# Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification

[**ICCV 2015**](https://arxiv.org/pdf/1502.01852.pdf)

## Abstract

**ì •êµí•œ activation unit(rectifiers)**ê°€ neural networkì— í•„ìˆ˜ì ì´ê³ , ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì´ ë…¼ë¬¸ì—ì„œ ë‘ ê°€ì§€ ë©´ì„ í†µí•´ ì •êµí•œ neural networkì— ëŒ€í•´ ì´ì•¼ê¸°í•¨.

1. **PReLU**ë¥¼ ê³ ì•ˆí•¨: generalize the traditional rectified unit
    1. extra computational cost ê±°ì˜ ë“¤ì§€ ì•ŠìŒ + overfitting risk ì‘ìŒ â†’ model fitting ê°œì„ 
2. **Robust initialization method** ê³ ì•ˆí•¨: (ìš°ë¦¬ê°€ ì•„ëŠ” He Initialization!) ì •êµí•œ nonlinearities ê³ ë ¤í•¨

## Introduction

ì´ ë…¼ë¬¸ ì „ì˜ recognition taskëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ í†µí•´ ì„±ëŠ¥ì„ ê°œì„ í•¨.

1. ë” ê°•ë ¥í•œ model ìì²´ë¥¼ build
2. overfittingì„ ë°©ì§€í•˜ëŠ” íš¨ìœ¨ì ì¸ strategiesë¥¼ ë””ìì¸

ê·¸ëŸ°ë° ì´ì— ë”°ë¼ complexityê°€ ì¦ê°€(depthì˜ ì¦ê°€, widthì˜ ì¦ê°€, ë” ì‘ì€ strideì˜ ì‚¬ìš©, ìƒˆë¡œìš´ nonlinear activations, ë³µì¡í•œ layer design..)ë˜ì–´ NNì€ training dataì— ì ì  ë” fittingí•˜ê²Œ ë¨.

ê·¸ë¦¬ê³  ë™ì‹œì— ë” ë‚˜ì€ generalizationì´ íš¨ìœ¨ì ì¸ ì •ê·œí™” techniques, ê°•ë ¥í•œ ë°ì´í„° ì¦ê°•, ê·¸ë¦¬ê³  í° ìŠ¤ì¼€ì¼ì˜ ë°ì´í„°ì— ì˜í•´ ë‹¬ì„±ë˜ê¸°ë„ í•¨.

 ì´ëŸ¬í•œ ë‹¹ì‹œì˜ ê¹Šì€ networkì˜ ë‹¬ì„±ì—ëŠ” **ReLU**(Rectified Linear Unit)ì˜ ê³µë„ ìˆì—ˆìŒ. ì´ê²ƒì€ ê¸°ì¡´ì— ì“°ì´ë˜ sigmoidë³´ë‹¤ training ê³¼ì •ì—ì„œ ë” convergeë¥¼ ì´‰ì§„í•˜ê³ , ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë„ì¶œí•˜ê²Œ í•¨. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  rectifierì— ëŒ€í•œ ê³ ë ¤ëŠ” model ì„¤ê³„ ê³¼ì •ì—ì„œ ì¶©ë¶„íˆ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ìŒ.

 ê·¸ë˜ì„œ ì´ ë…¼ë¬¸ì—ì„œëŠ” rectifiersì— ì˜í•´ ë„ì¶œë˜ëŠ” ë‘ ê°€ì§€ ì¢‹ì€ ë©´ì„ ê³ ë ¤í•´ì„œ NNì„ ëœ¯ì–´ë´„. 

1. ReLUì˜ generalizationëœ í˜•íƒœì˜ activation functionì¸ PReLU
    1. ì´ activation functionì€ recitifersë¡œë¶€í„° parameterë¥¼ learní•¨
    2. ê·¸ë¦¬ê³  ì¶”ê°€ì ì¸ computational ë¹„ìš© ì—†ì´ ì •í™•ë„(ì„±ëŠ¥)ì„ ì¦ê°€ì‹œí‚´
2. ë§¤ìš° ê¹Šì€ rectified modelì„ trainí•˜ëŠ” ë°ì— ì–´ë ¤ì›€ì„ ëŠë‚Œ â†’ nonlinearity ê³ ë ¤í•œ ëª¨ë¸ë§ â†’ initialization methodë¥¼ ê³ ë ¤í•¨
    1. scratch ë‹¨ê³„ì—ì„œë¶€í„° ì‹œì‘ë˜ëŠ” ë§¤ìš° ê¹Šì€ ëª¨ë¸ì˜ convergenceë¥¼ ë³´ë‹¤ ì‰½ë„ë¡ ë„ì›€
    2. ë” ê°•ë ¥í•œ network architectureë¥¼ íƒìƒ‰í•˜ëŠ” ë°ì— flexibilityë¥¼ ë¶€ì—¬í•¨

## Approach

### Parametric Rectifiers

![Untitled](20220317_i%2096eaf/Untitled.png)

ë¶„ë¥˜ì˜ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ **parameter-free ReLU activationì„ learned parametric activation unitë¡œ ë°”ê¿ˆ**

**ğŸ“ŒÂ Definition**

![Untitled](20220317_i%2096eaf/Untitled%201.png)

- yi: ië²ˆì§¸ channelì—ì„œì˜ input of the nonlinear activation f
- ai: ìŒì˜ ë¶€ë¶„ì—ì„œ slopeë¥¼ ì»¨íŠ¸ë¡¤
    - ai == 0ì´ ë˜ë©´ ê·¸ê²Œ ë°”ë¡œ ReLU
    - **aiì´ learnable parameterê°€ ë˜ë©´ ê·¸ê²ƒì´ ë°”ë¡œ PReLU**

â†’ ì´ ë…¼ë¬¸ì—ì„œëŠ” **channelë§ˆë‹¤ ë‹¤ë¥¸ nonlinear activation**ì„ ê°€ëŠ¥í•˜ë„ë¡ í•¨

- ë§Œì•½ aiê°€ small & fixedí•œ ê°’ì´ë¼ë©´:
    - ai == 0.01ì¸ ê³³ì—ì„œ LReLU
        - LReLUì˜ ì•„ì´ë””ì–´ëŠ” zero gradientë¥¼ ë°©ì§€í•˜ìëŠ” ê²ƒ
        - ê·¸ëŸ°ë° ReLUë‘ ë¹„êµí–ˆì„ ë•Œ ê·¸ë ‡ê²Œ í° ì„±ëŠ¥ í–¥ìƒì´ ì´ë£¨ì–´ì§€ì§€ëŠ” ì•ŠìŒ
    
    ë°˜ë©´ PReLUëŠ” ì „ ëª¨ë¸ì— ê±¸ì³ adaptivelyí•˜ê²Œ PReLU parameterë¥¼ í•™ìŠµí•¨ â†’ ë” specializedí•œ activationì„ ë„ì¶œí•˜ê¸° ìœ„í•´ end-to-end trainingì„ í¬ë§
    
- ì•„ì£¼ ì‘ì€ ìˆ˜ì˜ extra parameterì„ ìš”êµ¬
    - ì´ ìˆ˜ëŠ” channelì˜ ìˆ˜ì™€ ê°™ìŒ â†’ weightì˜ ìˆ˜ì— ë¹„í•˜ë©´ ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” ê°’
        - ê·¸ë˜ì„œ overfittingì˜ ë¦¬ìŠ¤í¬ëŠ” ì—†ë‹¤ê³  ë´„ **(ê·¼ë° ì´ê²Œ ë¬´ìŠ¨ ì¸ê³¼ê´€ê³„ì¸ì§€ëŠ”..)**
    - channel-shared variant ê³ ì•ˆ
        - coefficientëŠ” one layerì˜ ëª¨ë“  channelì— ëŒ€í•´ ê³µìœ ë¨
        - ê·¸ë˜ì„œ ê° layerì— ëŒ€í•´ ì˜¤ì§ í•˜ë‚˜ì˜ ì¶”ê°€ì ì¸ parameterë§Œ ìš”êµ¬í•¨

**ğŸ“ŒÂ Optimization**

PReLUëŠ” backpropagationì„ í†µí•´ trianë˜ê³ , other layersì™€ ë™ê¸°ì ìœ¼ë¡œ optimizedë¨.

![Untitled](20220317_i%2096eaf/Untitled.jpeg)

- ai updateì— ê³ ë ¤/ì´ìš©ë˜ì§€ ì•Šì€ ê²ƒ
    - **weight decay:** weight decayëŠ” aiê°€ 0ì´ ë˜ë„ë¡ ê°•ì œí•˜ê¸° ë•Œë¬¸ì— ReLUë‘ ë¹„ìŠ·í•´ì§
    - **regularization:** ì•ˆ ì‚¬ìš©í•´ë„ 1ì„ ë„˜ì–´ê°€ì§€ ì•Šì•˜ìŒ
    - **constrain of the range:** non-monotonic(ë‹¨ì¡°ë¡­ì§€ ì•Šë„ë¡)í•œ activation functionì´ ë˜ë„ë¡ aiì˜ ë²”ìœ„ë¥¼ ì œí•œí•˜ì§€ ì•ŠìŒ

**ğŸ“ŒÂ Comparison Experiments**

- ê¹Šê³  íš¨ìœ¨ì ì¸ 14 weight layer modelì— ëŒ€í•´ì„œ ë¹„êµ ì§„í–‰
    
    filter size, filter number listëŠ” table 1 ì°¸ê³ 
    
    ![Untitled](20220317_i%2096eaf/Untitled%202.png)
    
- **baseline**: ReLUë¥¼ conv layerì™€ ì²« 2ê°œì˜ fc layerì— ì ìš©ì‹œí‚´
    - ImageNet 2021 10-view testingì— ëŒ€í•´
        - top-1 errors: 33.82%
        - top-5 errors: 13.34%
- **compare**: ReLUë¥¼ PReLUë¡œ ë°”ê¿”ì„œ scratchë‹¨ê³„ë¶€í„° ë‹¤ì‹œ trainì‹œí‚´
    - top-1 errorëŠ” 32.64%ë¡œ 1.2% ë” ê°œì„ ë¨

![Untitled](20220317_i%2096eaf/Untitled%203.png)

 ìœ„ table 2ì—ì„œëŠ” channel-wise/channel-shared PReLUë„ ì„±ëŠ¥ì ìœ¼ë¡œ ë¹„êµí•  ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ì‹œì‚¬í•¨. 

- channel-shared version
    - ReLUì— ë¹„í•´ 13ê°œì˜ parameterë§Œ ë” ìƒê¹€
    - ê·¸ëŸ°ë° ì´ëŸ° ì‘ì€ ì°¨ë„ 1.1% ì •ë„ì˜ ì„±ëŠ¥ ê°œì„ ì— í¬ê²Œ ê¸°ì—¬í•¨
    - ì´ëŸ° ì ì€ activation functionì—ì„œì˜ adaptively learningì˜ ì¤‘ìš”ì„±ì„ ë³´ì—¬ ì¤Œ

 ê·¸ë¦¬ê³  ì´ì— ëŒ€í•´ table 1ì—ì„œ ê´€ì°°ë˜ëŠ” ë‘ ê°€ì§€ í¥ë¯¸ë¡œìš´ ì ì´ ìˆìŒ

1. conv1: coefficients 0.681, 0.596: ëª¨ë‘ 1ë³´ë‹¤ í™•ì‹¤íˆ í¼
    1. conv1ì€ edge, texture dectector â†’ **positive, negative ì„±ì§ˆ ëª¨ë‘** ë³´ì¡´ë¨
    2. í•„í„° ìˆ˜ê°€ ì œí•œë˜ì–´ ìˆëŠ” ê²½ìš° íš¨ìœ¨ì ì„
2. channel-wise versionì—ì„œ ë” ê¹Šì€ conv layerëŠ” ì‘ì€ coefficientsë¥¼ ê°€ì§
    1. activationì´ ê¹Šì´ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ë” nonlinearí•´ì¡Œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•¨ (ì™œëƒë©´ ì–‘ìˆ˜ì¼ ë•Œì˜ scopeê°€ 1ì´ë¯€ë¡œ)
    2. ê·¸ëŸ¬ë‹ˆê¹Œ **ê¹Šì´ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ì „ ë‹¨ê³„ì˜ ì •ë³´ë¥¼ ë” keep**í•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤ëŠ” ì˜ë¯¸

### Initialization of Filter Weights for Rectifiers

Recitifer networkëŠ” ê¸°ë³¸ì ìœ¼ë¡œ sigmoid-like activation networkë³´ë‹¤ëŠ” trainí•˜ê¸° ì‰½ì§€ë§Œ ë‚˜ìœ initizalizationì€ non-linearí•œ systemì˜ learningì„ ë§‰ì„ ìˆ˜ ìˆìŒ. ê·¸ë˜ì„œ ì•„ì£¼ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ trainì˜ ê±¸ë¦¼ëŒì„ ì œê±°í•˜ë„ë¡ **robust**í•œ initialization methodë¥¼ ê³ ì•ˆí•˜ê³ ì í•¨.

- ê¸°ì¡´ì˜ initialization
    - ê°€ìš°ì‹œì•ˆ ë¶„í¬ì— ì˜í•œ random weights drawn
    - fixed standard deviations â†’ ê¹Šì€ modelë“¤ì€ convergeì— ì–´ë ¤ì›€ì„ ê°€ì§
        - ì²« 8ê°œì˜ layerì— ëŒ€í•´ pre-train â†’ deeper modelì„ initialization
        - ê·¸ëŸ¬ë‚˜ train timeì´ ë§ì´ ë“¤ê³  poorer local optimumì„ ë„ì¶œì‹œí‚´
        - auxiliary classificationì´ ì¤‘ê°„ layerì˜ convergeë¥¼ ë•ê¸° ìœ„í•´ ì¶”ê°€ë˜ê¸°ë„..
    - **xavier**
        - **activationì´ linearí•˜ë‹¤ëŠ” ê°€ì • ì•„ë˜ ë§Œë“¤ì–´ì§**
        - ReLUë‚˜ PReLU ê°™ì€ nonlinearí•œ activationì—ëŠ” ë¶€ì ì ˆí•¨
        - ê·¸ë¦¬ê³  ì—„ì²­ deepí•œ ëª¨ë¸ì—ëŠ” ë¶€ì ì ˆí•¨
        

**ğŸ“ŒÂ Forward Propagation Case**

 each layerì—ì„œì˜ responseì˜ **variance**ë¥¼ ë³´ëŠ” ê²ƒì´ ì¼ë‹¨ main idea

- conv layerì—ì„œì˜ response:
    - *x*: *(k^2)c-by-1* vector
        - c input channelì— ëŒ€í•œ kxkê°œì˜ pixel í‘œí˜„
        - n = (k^2)c : responseì˜ connectionì˜ ìˆ«ì
    - *W: d-by-n* matrix
        - d: W rowì˜ filterì˜ ê°¯ìˆ˜
    - *b*: vector of biases
    - *y*: output mapì˜ pixelì—ì„œì˜ response
    - *l*: layerì˜ index

![Untitled](20220317_i%2096eaf/Untitled%204.png)

- xl = f(y_(l-1))ì—ì„œ fê°€ activation
- cl = d_(l-1)

*Wlì˜ ì´ˆê¸°í™”ëœ ìš”ì†Œë“¤*: independent & share the same distribution (Xl are also)

ê·¸ë¦¬ê³  Wl, Xl: independent of each other

ê·¸ë ‡ê²Œ í•´ì„œ:

(ì°¸ê³ : [https://medium.com/@shoray.goel/kaiming-he-initialization-a8d9ed0b5899](https://medium.com/@shoray.goel/kaiming-he-initialization-a8d9ed0b5899))

![Untitled](20220317_i%2096eaf/Untitled%205.png)

![Untitled](20220317_i%2096eaf/Untitled%206.png)

yl, xl, wlì€ ê°ê°ì˜ random variableì„ ë‚˜íƒ€ëƒ„. wlì€ zero meanì„ ê°€ì§€ê³  ìˆìŒ. ê·¸ë¦¬ê³  **independentí•œ variableì˜ ê³±**ì´ë¯€ë¡œ ë‹¤ìŒ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒ.

![Untitled](20220317_i%2096eaf/Untitled%207.png)

![Untitled](20220317_i%2096eaf/Untitled%208.png)

 ì—¬ê¸°ì„œ EëŠ” ê¸°ëŒ“ê°’. xlì˜ í‰ê· ì´ 0ì´ ì•„ë‹Œ ì´ìƒ E[xl^2] â‰  Var[x1]ì´ë¼ëŠ” ì ì„ ìœ ì˜í•´ì•¼ í•¨. 

ReLUì—ì„œëŠ” xl = max(0, y_(l-1))ì´ê³  ì´ê²ƒì€ zero meanì„ ê°€ì§ˆ ìˆ˜ ì—†ìŒ. 

ë§Œì•½ w_(l-1)ì´ 0ì„ ì¤‘ì‹¬ìœ¼ë¡œ ëŒ€ì¹­ì ì¸ ë¶„í¬ë¥¼ ê°€ì§„ë‹¤ë©´, y_(l-1) ë˜í•œ zero meanì„ ê°€ì§€ë©° 0ì„ ì¤‘ì‹¬ìœ¼ë¡œ ëŒ€ì¹­ì ì¸ ë¶„í¬ë¥¼ ê°€ì§„ë‹¤. ì´ëŠ” fê°€ ReLUì¼ ë•Œ

![Untitled](20220317_i%2096eaf/Untitled%209.png)

ë¼ëŠ” ê²°ë¡ ì„ ë„ì¶œì‹œí‚¤ê³ , ì´ë¥¼ ìœ„ ì‹ì— ëŒ€ì…ì‹œí‚¤ë©´

![Untitled](20220317_i%2096eaf/Untitled%2010.png)

ì´ëŸ¬í•œ ì‹ì„ ë„ì¶œì‹œí‚¬ ìˆ˜ ìˆë‹¤. ì´ê²ƒì— ëŒ€í•´ Lê°œì˜ layerê°€ í•¨ê»˜ ì§„í–‰ë˜ë©´: 

![Untitled](20220317_i%2096eaf/Untitled%2011.png)

ì´ ì‹ì„ ì–»ëŠ”ë‹¤. 

ì´ ë¶„ì‚°ì€ initialization ì„¤ê³„ì˜ í•µì‹¬ì¸ë°, **proper initializationì€ input signalì„ ì—„ì²­ ê°ì†Œì‹œí‚¤ê±°ë‚˜ í˜¹ì€ ì¦ê°€ì‹œí‚¤ëŠ” ê²ƒì„ í”¼í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—** proper scalarì„ ì„¤ì •í•´ì•¼ í•¨.

![Untitled](20220317_i%2096eaf/Untitled%2012.png)

 ê·¸ë˜ì„œ ì´ê²Œ ê°€ì¥ ì ì ˆí•˜ê³ , ì´ëŠ” stdê°€ (2/nl)**(1/2)ì¸ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¦„. ì´ê²ƒì´ initialization method. ê·¸ë¦¬ê³  l=1ì¼ ë•ŒëŠ” inputì— ReLUê°€ ì ìš©ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ n1Var[w1] = 1ì„. ê·¼ë° 1/2ë¼ëŠ” factorëŠ” ì²«ì§¸ ë ˆì´ì–´ì—ì„œ í•˜ë“ ì§€ ë§ë“ ì§€ ë³„ ìƒê´€ì´ ì—†ê³  ì—¬ê¸°ì„œëŠ” ì ìš©ì‹œí‚¨ë‹¤ê³  í•¨. (all)

***plus) pythonì—ì„œì˜ êµ¬í˜„..***

![Untitled](20220317_i%2096eaf/Untitled%2013.png)

**ğŸ“ŒÂ Backward Propagation Case**

(ê³„ì‚°ì„ ê±°ì³..)

ê²°ë¡ : 

![Untitled](20220317_i%2096eaf/Untitled%2012.png)

**ğŸ“ŒÂ Discussions**

- PReLUì—ì„œëŠ”

![Untitled](20220317_i%2096eaf/Untitled%2014.png)

ì´ë ‡ê²Œ ì´ˆê¸°í™”ì‹œí‚¤ê³ , aëŠ” coefficientì˜ initialized valueì„

**a = 0ì´ë©´ ReLU caseì™€ ë™ì¼í•´ì§.**

**a = 1ì´ë©´, linear caseì™€ ê°™ì•„ì§.**

**ğŸ“ŒÂ Comparisons with â€œXavierâ€ Initialization**

- Xavier:

![Untitled](20220317_i%2096eaf/Untitled%2015.png)

- He:

![Untitled](20220317_i%2096eaf/Untitled%2016.png)

ê°€ì¥ í° ì°¨ì´ì ì€ **nonlinearlity**ì— ëŒ€í•œ ì ‘ê·¼. Xavierì˜ derivationì€ linear caseì— ëŒ€í•´ì„œë§Œ ê³ ë ¤í•¨. 

Xavierì€ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì—ì„œ stdê°€ (1/nl)**(1/2)ë¡œ í‘œí˜„ë˜ëŠ”ë°, ì´ê²ƒì˜ stdëŠ” Heì˜ stdì˜ (1/(2)**(L/2))ê°€ ë¨. **ì´ê²ƒì€ convergeí•˜ê¸°ì—ëŠ” ë„ˆë¬´ ì‘ìŒ**. 

![Untitled](20220317_i%2096eaf/Untitled%2017.png)

22ê°œì˜ layerê°€ ìˆì„ ë•ŒëŠ” ë‘˜ ë‹¤ convergeí•˜ê¸°ëŠ” í•¨. ê·¸ëŸ¬ë‚˜ Heê°€ ì¡°ê¸ˆ ë” ë¹¨ë¦¬ errorì„ ê°ì†Œì‹œí‚¤ê³  convergeí•¨. ê·¸ë¦¬ê³  ì •í™•ë„ë„ Heê°€ ë” ë‚˜ì•˜ìŒ. 

![Untitled](20220317_i%2096eaf/Untitled%2018.png)

ì¢€ ë” ê¹Šì€ ëª¨ë¸ì—ì„œëŠ”(16 conv layer, 256 2x2 filters) ì´ì œ 30-layer ëª¨ë¸ì¸ë°, XavierëŠ” stallsí•¨. The gradientsê°€ ì‚¬ë¼ì§(gradient diminishing)

## Experiments on ImageNet

- Comparisons between ReLU and PReLU: PReLUê°€ computional costì˜ ì¦ê°€ ì—†ì´ small/large model ëª¨ë‘ì—ì„œ accuracyë¥¼ improveì‹œí‚´

![Untitled](20220317_i%2096eaf/Untitled%2019.png)

- Comparisons of Single-model Results & Multi-model Results:

![Untitled](20220317_i%2096eaf/Untitled%2020.png)
