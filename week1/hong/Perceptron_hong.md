# 퍼셉트론

## 개념

- 다수의 신호를 입력으로 받아 하나의 신호를 출력한다.
- 인공 신경망의 기원이 되는 알고리즘이다.
- 입력층(input layer), 은닉층(hidden layer), 출력층(output layer)으로 구성되어있다.
- 단순 퍼셉트론과 다층 퍼셉트론(MLP)가 있는데, 은닉층이 없는 퍼셉트론을 단순 퍼셉트론이라 하고, 은닉층이 하나 이상인 퍼셉트론을 다층 퍼셉트론이라고 한다. 단순 퍼셉트론의 경우 XOR게이트를 구현할 수 없다.
- 다층 퍼셉트론의 경우 fully connection layer(DNN)와 거의 똑같으나, 활성화함수(activation function)가 계단함수(상수함수)라는 점만 다르다.

## 식

![image](https://user-images.githubusercontent.com/61305409/155904962-7509d331-71fd-4632-accf-efd2ccdae625.png)

- 위의 그림은 다층 퍼셉트론에서 입력층-은닉층의 연산을 그래프로 나타낸 것이다.
- 연산은 입력층-은닉층과 은닉층-출력층, 혹은 단순 퍼셉트론의 입력층-출력층 모두 같은 형태(y=Wx+b)로 되어있다.
- 위의 그림에서 W 행렬의 표기에서 원래의 표기에서 행과 열이 바뀌어 있는데, 원래의 표기를 유지하기 위해 $y = W^Tx+b$으로 식을 표기하기도 한다.
- W와 x는 행렵 곱으로 연결되어있고, 이런 행렬곱을 affine transformation(어파인 변환)이라고 한다.
- x의 dx1 shape에서 d는 입력값의 차원을 말하고, 1은 batch 크기를 말한다.
- MLP의 경우 활성화함수가 계단함수이므로 각 행의 연산 값인 $s = W_{11}x_1+W_{21}x_2+...+W_{d1}x_d+b_1$ 값이 0보다 작거나 같으면 0, 그렇지 않으면 1로 바꾼 y값을 다음 계층의 입력값으로 사용하여 같은 연산을 반복한다.

![image](https://user-images.githubusercontent.com/61305409/155904976-f469591e-e3bd-4169-9afb-0400b9a64433.png)
