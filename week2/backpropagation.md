# 활성화함수

### MLP의 등장

하나의 퍼셉트론으로는 XOR 게이트를 표현할 수가 없다. 대신 OR 게이트와 AND게이트를 조합하여 XOR 게이트를 표현할 수 있다. 즉, 퍼셉트론을 여러개 사용하면 XOR게이트를 표현할 수 있다.

### 활성화함수를 사용하는 이유

그렇다면 퍼셉트론을 여러개 이어붙이면 XOR게이트보다 훨씬 복잡한 함수들도 표현할 수 있을까? 아쉽게도 퍼셉트론을 여러 개 쌓는 것만으로는 부족하다. 왜냐하면 퍼셉트론을 2개 이어붙일 때 첫 번째 계층의 식은 $y_1=W_1x+b_1$ (단, $x$는 입력값, $W_1$는 첫번째 계층의 가중치 값, $b_1$는 첫번째 계층의 편향값, $y_1$는 첫번째 계층의 출력값) 가 되고 두번째 계층의 식은 비슷하게 $o=W_2y_1+b_2$ (단, $y_1$는 첫번째 계층으로부터 받은 입력값, $W_2$는 두번째 계층의 가중치 값, $b_2$는 두번째 계층의 편향값, $o$는 두번째 계층의 출력값)이 된다. 두 식을 대입을 통해 입력값과 최종 출력값으로 식을 나타내보면 $o=W_2(W_1x+b_1)+b_2 =W_2W_1x+W_2b_1+b_2$으로 x에 대해 일차식이 생성된다. 따라서 퍼셉트론을 아무리 많이 쌓아봤자 선형함수만 표현할 수 있다는 한계가 있다. 

더 복잡한 함수를 표현할 수 있게 하기 위해서 각 계층에서 계산한 값에 비선형함수를 적용하여 값에 비선형성을 부여하는데, 시그모이드 함수, 소프트맥스 함수 등을 주로 사용한다. 이렇게 적용하는 비선형함수를 활성화함수라고 한다. 활성화함수를 $f(.)$로 표현하면 위의 두 계층의 식은 다음으로 표현된다. $y_1=f(W_1x+b_1)$, $o=f(W_2y_1+b_2)$ 각 계층에 적용되는 활성화함수가 같을 필요는 없다. 

비선형의 활성화함수를 각 계층에 적용하면 은닉층이 1개인 MLP로도 어떤 복잡한 함수도 표현할 수 있다고 한다.

# 오차역전파

### 손실함수

손실함수는 모델이 예측한 함수가 최적의 함수에 얼마나 가까운지를 수치로 나타내기 위한 함수이다. 손실값이 크면 실제 함수값과는 많이 다른 함수를 예측하게 된 것이고 반대의 경우 비슷한 함수를 예측하게 된 것이다.

### 오차역전파

오차역전파가 고안되기 전의 인공신경망은 어떤 값들에 대한 함수를 예측하고 이를 이용하여 출력값을 계산하는 것에 그쳤다. 그러나 더 복잡한 함수를 나타내기 위해서는 인공 신경망을 더 좋은 값으로 학습해야만 하는데, 얼마나 값이 잘못 되었는지에 대한 값(손실값)을 역으로 앞방향으로 보내, 이 값을 참고하여 가중치값과 편향값을 수정한다. 따라서 손실함수(손실값)는 가중치와 편향을 어느정도 수정해야 하는지를 계산하기 위해 필요한 값이라 할 수 있다. 얼마나 수정해야 하는지는 손실함수의 미분값을 통해, 어떻게 수정해야 하는지는 최적화 알고리즘을 통해 계산한다. 최적화 알고리즘으로는 유명한 것으로는 gradient descent, Adam과 같은 방법이 있다. 

다음과 같은 2계층 짜리의 MLP에서의 오차역전파를 위한 미분식을 살펴보자.

![image](https://user-images.githubusercontent.com/61305409/157629760-3c3ee4c3-1942-49e7-a70e-3b17b2b3b27f.png)  

먼저 두번째 계층의 가중치에 대한 미분식을 볼텐데, 그 전에 두번째 계층의 계산식은 다음과 같으므로

![image](https://user-images.githubusercontent.com/61305409/157629728-b95738cb-0410-492e-b002-305120df0173.png)  

두번째 계층의 가중치에 대한 손실함수의 미분 식은 다음과 같다.

![image](https://user-images.githubusercontent.com/61305409/157629802-d6284043-c66c-4ebb-83e6-4b1b677284b6.png)   

두번째 계층에 대한 미분식을 이용하여 첫번째 계층의 가중치에 대한 손실함수의 미분을 계산한다. 

첫번째 계층부터 최종 결과값까지의 식은 아래와 같고,

![image](https://user-images.githubusercontent.com/61305409/157629865-b6691e8a-582f-4d86-9bd4-3262d654fbae.png)  
이를 이용하여 첫번째 가중치에 대한 손실함수의 미분식은 다음과 같다.

![image](https://user-images.githubusercontent.com/61305409/157629900-49f00d0e-570f-4370-ae3a-667b445ff8c1.png)  

위에서 계산한 값을 사용하여 가중치 값을 gradient descent를 최적화 알고리즘으로 사용한다면 아래의 식과 같이 갱신한다.

![image](https://user-images.githubusercontent.com/61305409/157629932-de9d7351-300e-466a-834c-8c60f9a68904.png)  
